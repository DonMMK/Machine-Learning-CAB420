{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d587852",
   "metadata": {},
   "source": [
    "# CAB420 DCNNs, Bonus Example: DNNs, Parameters and FLOPs\n",
    "Dr Simon Denman (s.denman@qut.edu.au)\n",
    "\n",
    "### What is a \"Bonus\" Example?\n",
    "\n",
    "These are extra examples that cover content outside the scope of CAB420. It exists becuase of one or more of the following reasons:\n",
    "* It's closely related to other stuff we're looking at and I wanted to include it, but the course has too much content already, so I punted it here; \n",
    "* It's interesting;\n",
    "* Someone (probably multiple someones if I wrote an example) has asked a question about it before.\n",
    "\n",
    "You can freely ignore this example if you want. You really don't have to be reading this. You could go outside, go read a book, have a nap, take up a hobby, whatever you want really. The point I want to make here **this example really is optional**. Things here won't appear on an exam, or in an assignment (though you could use this in an assignment if you wanted). But if you're interested, this is here, and if you're reading this, so are you. \n",
    "\n",
    "Some things to note with bonus examples:\n",
    "* These may gloss over details that elsewhere get more coverage. I may skip plots I'd normally include, or gloss over other details. The expecatation is that if you're reading this, you've looked at all the \"core\" examples and are comfortable with what they're doing. \n",
    "* Some bits of code might not be as well explained or explored as you're used to in the other examples. These examples are here for interested students looking to extend their knowledge, and I'm assuming if you're here, you're comfortable figuring code out, debugging stuff, and generally googling about to help work out what something is doing.\n",
    "* There's no Tl;DR section at the top. If you're here, I'm assuming it's because you're interested and want all the gory details and don't just need the quick summary at the top.\n",
    "* While my regular examples (the \"core\" ones) certainly contain their fare share of silly remarks and typos, expect the level of flippancy and the prevalence of typos increase in a bonus example. \n",
    "\n",
    "That said, as always, if you are stuggling to follow what I've got in here please shoot me a message. The aim is still for this to be clear enough to follow afterall.\n",
    "\n",
    "## Overview\n",
    "\n",
    "DCNNs are computationally expensive. They also result in lots of parameters. But from a computational demand standpoint, not all parameters are equal. \n",
    "\n",
    "This example has a little explore at how many FLOPs (floating point operations) are in a DCNN, and the cost of different types of layers, impact of input size, etc. To do this, we'll create a few networks, but:\n",
    "* We're not going to load any data\n",
    "* We're not going to train any networks\n",
    "\n",
    "We're just going to create the network, and then run some analysis over it.\n",
    "\n",
    "### Where does this fit into all the other CAB420 content?\n",
    "\n",
    "This complements the other DCNN content. It doesn't tie directly into any other single example, but just probes a bit deeper into the computational demands of DCNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d13a80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import datetime\n",
    "import numpy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorboard import notebook\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b032f",
   "metadata": {},
   "source": [
    "### Computing FLOPs\n",
    "\n",
    "Tensorflow has a fairly serious profiler in it, but as best I can tell (at the time of writing anyway) this doesn't have a nice simple way to just pull out the total FLOPs. Luckily, other people have written such code, and I've grabbed the below from the [keras flops package](https://github.com/tokusumi/keras-flops/blob/master/keras_flops/flops_calculation.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74db1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\n",
    "from tensorflow.keras import Sequential, Model\n",
    "\n",
    "def get_flops(model, batch_size = None):\n",
    "    \"\"\"\n",
    "    Calculate FLOPS for tf.keras.Model or tf.keras.Sequential .\n",
    "    Ignore operations used in only training mode such as Initialization.\n",
    "    Use tf.profiler of tensorflow v1 api.\n",
    "    \"\"\"\n",
    "    if not isinstance(model, (Sequential, Model)):\n",
    "        raise KeyError(\n",
    "            \"model arguments must be tf.keras.Model or tf.keras.Sequential instanse\"\n",
    "        )\n",
    "\n",
    "    if batch_size is None:\n",
    "        batch_size = 1\n",
    "\n",
    "    # convert tf.keras model into frozen graph to count FLOPS about operations used at inference\n",
    "    # FLOPS depends on batch size\n",
    "    inputs = [\n",
    "        tf.TensorSpec([batch_size] + inp.shape[1:], inp.dtype) for inp in model.inputs\n",
    "    ]\n",
    "    real_model = tf.function(model).get_concrete_function(inputs)\n",
    "    frozen_func, _ = convert_variables_to_constants_v2_as_graph(real_model)\n",
    "\n",
    "    # Calculate FLOPS with tf.profiler\n",
    "    run_meta = tf.compat.v1.RunMetadata()\n",
    "    opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "    flops = tf.compat.v1.profiler.profile(\n",
    "        graph=frozen_func.graph, run_meta=run_meta, cmd=\"scope\", options=opts\n",
    "    )\n",
    "    \n",
    "    # TODO: show each FLOPS\n",
    "    return flops.total_float_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd379bb",
   "metadata": {},
   "source": [
    "## A Simple Network\n",
    "\n",
    "Let's look at a simple network with just a few dense layers. For now, we'll pretend that we're using Fashion MNIST and so assume an input size of $28 \\times 28$, or if we vectorise it (like we are going to pretend here) $784$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f9d3276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fashion_mnist_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               200960    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 218,058\n",
      "Trainable params: 218,058\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create an input, we need to specify the shape of the input, in this case it's a vectorised images with a 784 in length\n",
    "inputs = keras.Input(shape=(784,), name='img')\n",
    "# first layer, a dense layer with 64 units, and a relu activation. This layer recieves the 'inputs' layer as it's input\n",
    "x = layers.Dense(256, activation='relu')(inputs)\n",
    "# second layer, another dense layer, this layer recieves the output of the previous layer, 'x', as it's input\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "# output layer, length 10 units. This layer recieves the output of the previous layer, 'x', as it's input\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# create the model, the model is a collection of inputs and outputs, in our case there is one of each\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='fashion_mnist_model')\n",
    "# print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005763ce",
   "metadata": {},
   "source": [
    "We have $218,058$ parameters, most of this in the first dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b1b4c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/simon/venvs/cab420/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:3836: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/435.84k flops)\n",
      "  fashion_mnist_model/dense/MatMul (401.41k/401.41k flops)\n",
      "  fashion_mnist_model/dense_1/MatMul (32.77k/32.77k flops)\n",
      "  fashion_mnist_model/dense_2/MatMul (1.28k/1.28k flops)\n",
      "  fashion_mnist_model/dense/BiasAdd (256/256 flops)\n",
      "  fashion_mnist_model/dense_1/BiasAdd (64/64 flops)\n",
      "  fashion_mnist_model/dense_2/Softmax (50/50 flops)\n",
      "  fashion_mnist_model/dense_2/BiasAdd (10/10 flops)\n",
      "\n",
      "======================End of Report==========================\n"
     ]
    }
   ],
   "source": [
    "flops = get_flops(model, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb122c4",
   "metadata": {},
   "source": [
    "In running our analysis we get this little report above. This is not super intuitive, but does tell us how expensive every operation in the network is.\n",
    "\n",
    "Let's start by considering a dense layers. A dense layer does the following:\n",
    "\n",
    "$\\hat{x} = x \\times w + b$\n",
    "\n",
    "where $x$ is the input, $w$ is the learned weight matrix equal, $b$ is the learned bias, and $\\hat{x}$ is the model output. Remember that $w$ is of size $\\text{length}(x) \\times \\text{length}(\\hat{x})$, and $b$ is of size $\\text{length}(\\hat{x})$. Looking at the first dense layer (just called `dense`), we have two operations associated with it:\n",
    "```\n",
    "  fashion_mnist_model/dense/MatMul (401.41k/401.41k flops)\n",
    "  fashion_mnist_model/dense/BiasAdd (256/256 flops)\n",
    "```\n",
    "\n",
    "The first of these, the `MatMul` is a matrix multiplication, so is $x \\times w$. We can see that this is a fairly big operation, which makes sense as $w$ is going to be $768 \\times 256$. We see subsequent matrix multiplications are much lighter weight, as the weight matrices are much smaller. The second of is the `BiasAdd` and is simply the $+ b$ part of the equation. Unsurprisingly, this element wise addition is pretty light weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7df088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.436 Million Floating Point Operations (FLOPs)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{flops / 10 ** 6:.03} Million Floating Point Operations (FLOPs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589ef4d",
   "metadata": {},
   "source": [
    "Overall, we a bit under half a billion FLOPs.\n",
    "\n",
    "### A DCNN\n",
    "\n",
    "Let's switch now to a DCNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f994afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fashion_mnist_cnn_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 26, 8)         80        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 32)          4640      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 288)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                18496     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,034\n",
      "Trainable params: 25,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# our input now has a different shape, 28x28x1, as we have 28x28 single channel images\n",
    "inputs = keras.Input(shape=(28, 28, 1, ), name='img')\n",
    "# rather than use a fully connected layer, we'll use 2D convolutional layers, 8 filters, 3x3 size kernels\n",
    "x = layers.Conv2D(filters=8, kernel_size=(3,3), activation='relu')(inputs)\n",
    "# 2x2 max pooling, this will downsample the image by a factor of two\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "# more convolution, 16 filters, followed by max poool\n",
    "x = layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu')(x)\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "# final convolution, 32 filters\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu')(x)\n",
    "# a flatten layer. Matlab does a flatten automatically, here we need to explicitly do this. Basically we're telling\n",
    "# keras to make the current network state into a 1D shape so we can pass it into a fully connected layer\n",
    "x = layers.Flatten()(x)\n",
    "# a single fully connected layer, 64 inputs\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "# and now our output, same as last time\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# build the model, and print the summary\n",
    "model_cnn = keras.Model(inputs=inputs, outputs=outputs, name='fashion_mnist_cnn_model')\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb87916",
   "metadata": {},
   "source": [
    "Parameter wise, we have gone from ~220,000 to ~25,000, but the FLOPs has not seem the same reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48d6538a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/511.98k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_1/Conv2D (278.78k/278.78k flops)\n",
      "  fashion_mnist_cnn_model/conv2d/Conv2D (97.34k/97.34k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_2/Conv2D (82.94k/82.94k flops)\n",
      "  fashion_mnist_cnn_model/dense_3/MatMul (36.86k/36.86k flops)\n",
      "  fashion_mnist_cnn_model/conv2d/BiasAdd (5.41k/5.41k flops)\n",
      "  fashion_mnist_cnn_model/max_pooling2d/MaxPool (5.41k/5.41k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_1/BiasAdd (1.94k/1.94k flops)\n",
      "  fashion_mnist_cnn_model/max_pooling2d_1/MaxPool (1.60k/1.60k flops)\n",
      "  fashion_mnist_cnn_model/dense_4/MatMul (1.28k/1.28k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_2/BiasAdd (288/288 flops)\n",
      "  fashion_mnist_cnn_model/dense_3/BiasAdd (64/64 flops)\n",
      "  fashion_mnist_cnn_model/dense_4/Softmax (50/50 flops)\n",
      "  fashion_mnist_cnn_model/dense_4/BiasAdd (10/10 flops)\n",
      "\n",
      "======================End of Report==========================\n"
     ]
    }
   ],
   "source": [
    "flops = get_flops(model_cnn, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641577ab",
   "metadata": {},
   "source": [
    "This output is a bit harder to unpick, but let's look at the first convolution operation. This has these the following entries:\n",
    "```\n",
    "  fashion_mnist_cnn_model/conv2d/Conv2D (97.34k/97.34k flops)\n",
    "  fashion_mnist_cnn_model/conv2d/BiasAdd (5.41k/5.41k flops)\n",
    "```\n",
    "We have the main convolution operation, and the bias operation. This first convolution has $80$ parameters, yet it has abouve $100,000$ FLOPs worth of compute associated with it. Why? Going back to the maths,\n",
    "\n",
    "$\\hat{x} = x * w + b$\n",
    "\n",
    "which looks a lot like our dense layer. Execpt, with convolution we're not just multiplying one thing by another. Instead, we are taking our $w$, which is generally (and certainly in our case) smaller than our $x$, and sliding this across $x$, evaluating the sum of the element wise product at each location. This leads to a lot of compute from a small number of parameters.\n",
    "\n",
    "The fact that our convolutions operate across multiple channels also means that we don't see the same decrease in parameters as we go deeper. Consider our second convolution operation, this has the following:\n",
    "```\n",
    "  fashion_mnist_cnn_model/conv2d_1/Conv2D (278.78k/278.78k flops)\n",
    "  fashion_mnist_cnn_model/conv2d_1/BiasAdd (1.94k/1.94k flops)\n",
    "```\n",
    "We have roughly three times the parameters in our second convolution compared to our first. This is despite the spatial resolution being reduced by the max pooling in the middle. The increase in the channel depth of the representation leads to this increase. By the time we get to the next convolution operation, the number of operations is starting to reduce again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a8d8181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.512 Million Floating Point Operations (FLOPs)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{flops / 10 ** 6:.03} Million Floating Point Operations (FLOPs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c3efe6",
   "metadata": {},
   "source": [
    "All up, we have just over half a billion FLOPs. We have a bit over 10% of the parameters, but see a slight increase in computational requirements. As mentioned above, not all parameters are created equal.\n",
    "\n",
    "### Bigger Inputs\n",
    "\n",
    "Let's repeat the above, but we'll increase the input sizes. We'll leave everything else the same. Let's assume that we've now got $50 \\times 50$ input images. \n",
    "\n",
    "#### Simple Dense Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50382e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fashion_mnist_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 2500)]            0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               640256    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 657,354\n",
      "Trainable params: 657,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create an input, we need to specify the shape of the input, in this case it's a vectorised images of length 2500\n",
    "inputs = keras.Input(shape=(2500,), name='img')\n",
    "# first layer, a dense layer with 64 units, and a relu activation. This layer recieves the 'inputs' layer as it's input\n",
    "x = layers.Dense(256, activation='relu')(inputs)\n",
    "# second layer, another dense layer, this layer recieves the output of the previous layer, 'x', as it's input\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "# output layer, length 10 units. This layer recieves the output of the previous layer, 'x', as it's input\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# create the model, the model is a collection of inputs and outputs, in our case there is one of each\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='fashion_mnist_model')\n",
    "# print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63a045ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/1.31m flops)\n",
      "  fashion_mnist_model/dense_5/MatMul (1.28m/1.28m flops)\n",
      "  fashion_mnist_model/dense_6/MatMul (32.77k/32.77k flops)\n",
      "  fashion_mnist_model/dense_7/MatMul (1.28k/1.28k flops)\n",
      "  fashion_mnist_model/dense_5/BiasAdd (256/256 flops)\n",
      "  fashion_mnist_model/dense_6/BiasAdd (64/64 flops)\n",
      "  fashion_mnist_model/dense_7/Softmax (50/50 flops)\n",
      "  fashion_mnist_model/dense_7/BiasAdd (10/10 flops)\n",
      "\n",
      "======================End of Report==========================\n"
     ]
    }
   ],
   "source": [
    "flops = get_flops(model, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffdaae26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.31 Million Floating Point Operations (FLOPs)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{flops / 10 ** 6:.03} Million Floating Point Operations (FLOPs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb8228",
   "metadata": {},
   "source": [
    "The number of parameters has trippled, though only the number of parameters in the first dense layer has changed. Looking at the FLOPs, overall we've gone up by a factor of roughly 2.5, though again all the increase has happened in the first dense layer, and only in the matrix multiplication component. Remember, this doing\n",
    "\n",
    "$\\hat{x} = x \\times w + b$.\n",
    "\n",
    "$w$ depends on the size of the input (which got bigger), and the output (which is unchanged). $b$ only depends on the size of the output. So our bias operation is unchanged, and our $w$ operation blows out. After this layer, we're back to the same size representation as we had before, so everything is unchanged.\n",
    "\n",
    "#### Simple DCNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bde4312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fashion_mnist_cnn_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 50, 50, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 48, 48, 8)         80        \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 24, 24, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 22, 22, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 11, 11, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 9, 9, 32)          4640      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2592)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                165952    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 172,490\n",
      "Trainable params: 172,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# pretending that we have 50x50 inputs\n",
    "inputs = keras.Input(shape=(50, 50, 1, ), name='img')\n",
    "# rather than use a fully connected layer, we'll use 2D convolutional layers, 8 filters, 3x3 size kernels\n",
    "x = layers.Conv2D(filters=8, kernel_size=(3,3), activation='relu')(inputs)\n",
    "# 2x2 max pooling, this will downsample the image by a factor of two\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "# more convolution, 16 filters, followed by max poool\n",
    "x = layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu')(x)\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "# final convolution, 32 filters\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu')(x)\n",
    "# a flatten layer. Matlab does a flatten automatically, here we need to explicitly do this. Basically we're telling\n",
    "# keras to make the current network state into a 1D shape so we can pass it into a fully connected layer\n",
    "x = layers.Flatten()(x)\n",
    "# a single fully connected layer, 64 inputs\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "# and now our output, same as last time\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# build the model, and print the summary\n",
    "model_cnn = keras.Model(inputs=inputs, outputs=outputs, name='fashion_mnist_cnn_model')\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2eded481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/2.58m flops)\n",
      "  fashion_mnist_cnn_model/conv2d_4/Conv2D (1.12m/1.12m flops)\n",
      "  fashion_mnist_cnn_model/conv2d_5/Conv2D (746.50k/746.50k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_3/Conv2D (331.78k/331.78k flops)\n",
      "  fashion_mnist_cnn_model/dense_8/MatMul (331.78k/331.78k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_3/BiasAdd (18.43k/18.43k flops)\n",
      "  fashion_mnist_cnn_model/max_pooling2d_2/MaxPool (18.43k/18.43k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_4/BiasAdd (7.74k/7.74k flops)\n",
      "  fashion_mnist_cnn_model/max_pooling2d_3/MaxPool (7.74k/7.74k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_5/BiasAdd (2.59k/2.59k flops)\n",
      "  fashion_mnist_cnn_model/dense_9/MatMul (1.28k/1.28k flops)\n",
      "  fashion_mnist_cnn_model/dense_8/BiasAdd (64/64 flops)\n",
      "  fashion_mnist_cnn_model/dense_9/Softmax (50/50 flops)\n",
      "  fashion_mnist_cnn_model/dense_9/BiasAdd (10/10 flops)\n",
      "\n",
      "======================End of Report==========================\n"
     ]
    }
   ],
   "source": [
    "flops = get_flops(model_cnn, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a02c3b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.58 Million Floating Point Operations (FLOPs)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{flops / 10 ** 6:.03} Million Floating Point Operations (FLOPs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6503e9",
   "metadata": {},
   "source": [
    "Here, we've got a bit of a blow out. Our parameters increased by a factor of about $7$, and our FLOPs have gone up by five. Parameter wise, we see our only change is in the dense layer after our thrid convolution which has gone right up (all but about 7,000 network parameters are in this layer). The FLOPs though have gone through the roof for the convolution operations.\n",
    "\n",
    "Again, let's consider what's going on. Our convolution operations are applying the convolution filter with a sliding window. A larger input means we apply the convolution at more locations. Compared to what we started with, we're going to applying our convolution kernel at $784$ locations (each of the pixels) to $2500$ locations. We see the FLOPs go up accordingly. As subsequent convolution layers are now also operating over representations that are larger spatially, we see a similar increase. \n",
    "\n",
    "When we get to the dense layer, the larger spatial representation means that when we flatten things, we get a larger matrix multiplication operation. Hence our weight matrix for this operation get's big, and we see the parameters increase massivley - though this is not where the bulk of the computational cost comes from.\n",
    "\n",
    "#### Adding an Extra Convolution + MaxPool\n",
    "\n",
    "Let's add another convolution and maxpool and see what happens. This will obviously add another convolution operation, but will also make the input to the dense layer much smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0153e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fashion_mnist_cnn_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 50, 50, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 48, 48, 8)         80        \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 24, 24, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 22, 22, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 11, 11, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 9, 9, 32)          4640      \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 4, 4, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 2, 2, 64)          18496     \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,482\n",
      "Trainable params: 41,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# pretending that we have 50x50 inputs\n",
    "inputs = keras.Input(shape=(50, 50, 1, ), name='img')\n",
    "# rather than use a fully connected layer, we'll use 2D convolutional layers, 8 filters, 3x3 size kernels\n",
    "x = layers.Conv2D(filters=8, kernel_size=(3,3), activation='relu')(inputs)\n",
    "# 2x2 max pooling, this will downsample the image by a factor of two\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "# more convolution, 16 filters, followed by max poool\n",
    "x = layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu')(x)\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "# third convolution, 32 filters\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu')(x)\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "# fourth convolution\n",
    "x = layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu')(x)\n",
    "# a flatten layer. Matlab does a flatten automatically, here we need to explicitly do this. Basically we're telling\n",
    "# keras to make the current network state into a 1D shape so we can pass it into a fully connected layer\n",
    "x = layers.Flatten()(x)\n",
    "# a single fully connected layer, 64 inputs\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "# and now our output, same as last time\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# build the model, and print the summary\n",
    "model_cnn = keras.Model(inputs=inputs, outputs=outputs, name='fashion_mnist_cnn_model')\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c31a308b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/2.43m flops)\n",
      "  fashion_mnist_cnn_model/conv2d_7/Conv2D (1.12m/1.12m flops)\n",
      "  fashion_mnist_cnn_model/conv2d_8/Conv2D (746.50k/746.50k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_6/Conv2D (331.78k/331.78k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_9/Conv2D (147.46k/147.46k flops)\n",
      "  fashion_mnist_cnn_model/dense_10/MatMul (32.77k/32.77k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_6/BiasAdd (18.43k/18.43k flops)\n",
      "  fashion_mnist_cnn_model/max_pooling2d_4/MaxPool (18.43k/18.43k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_7/BiasAdd (7.74k/7.74k flops)\n",
      "  fashion_mnist_cnn_model/max_pooling2d_5/MaxPool (7.74k/7.74k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_8/BiasAdd (2.59k/2.59k flops)\n",
      "  fashion_mnist_cnn_model/max_pooling2d_6/MaxPool (2.05k/2.05k flops)\n",
      "  fashion_mnist_cnn_model/dense_11/MatMul (1.28k/1.28k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_9/BiasAdd (256/256 flops)\n",
      "  fashion_mnist_cnn_model/dense_10/BiasAdd (64/64 flops)\n",
      "  fashion_mnist_cnn_model/dense_11/Softmax (50/50 flops)\n",
      "  fashion_mnist_cnn_model/dense_11/BiasAdd (10/10 flops)\n",
      "\n",
      "======================End of Report==========================\n"
     ]
    }
   ],
   "source": [
    "flops = get_flops(model_cnn, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86112410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.43 Million Floating Point Operations (FLOPs)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{flops / 10 ** 6:.03} Million Floating Point Operations (FLOPs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f6bce6",
   "metadata": {},
   "source": [
    "This has a perhaps unexpected impact. We have made our network deeper, yet greatly reduced the number of parameters, and reduced the number of FLOPs (slightly). Even with adding a 64 filter convolution layer here, I appear to have made the network simpler. \n",
    "\n",
    "This issue here is how these layers interact with each other. With deep nets, the output of one layer is the input to the next, so by manipulating sizes like this we can impact how efficient later parts of a network are. Does this mean that deepening a network is a path towards simplification? Not really, or at least not in most cases. This one here is a bit of an edge case. One way to have perhaps a larger impact is via global average pooling. Replacing my fourth convolution layer with this, I get the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f159618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fashion_mnist_cnn_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 50, 50, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 48, 48, 8)         80        \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 24, 24, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 22, 22, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 11, 11, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 9, 9, 32)          4640      \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 32)               0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,650\n",
      "Trainable params: 8,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# pretending that we have 50x50 inputs\n",
    "inputs = keras.Input(shape=(50, 50, 1, ), name='img')\n",
    "# rather than use a fully connected layer, we'll use 2D convolutional layers, 8 filters, 3x3 size kernels\n",
    "x = layers.Conv2D(filters=8, kernel_size=(3,3), activation='relu')(inputs)\n",
    "# 2x2 max pooling, this will downsample the image by a factor of two\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "# more convolution, 16 filters, followed by max poool\n",
    "x = layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu')(x)\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "# third convolution, 32 filters\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu')(x)\n",
    "# a flatten layer. Matlab does a flatten automatically, here we need to explicitly do this. Basically we're telling\n",
    "# keras to make the current network state into a 1D shape so we can pass it into a fully connected layer\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "# a single fully connected layer, 64 inputs\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "# and now our output, same as last time\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# build the model, and print the summary\n",
    "model_cnn = keras.Model(inputs=inputs, outputs=outputs, name='fashion_mnist_cnn_model')\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19a1163d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/2.26m flops)\n",
      "  fashion_mnist_cnn_model/conv2d_11/Conv2D (1.12m/1.12m flops)\n",
      "  fashion_mnist_cnn_model/conv2d_12/Conv2D (746.50k/746.50k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_10/Conv2D (331.78k/331.78k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_10/BiasAdd (18.43k/18.43k flops)\n",
      "  fashion_mnist_cnn_model/max_pooling2d_7/MaxPool (18.43k/18.43k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_11/BiasAdd (7.74k/7.74k flops)\n",
      "  fashion_mnist_cnn_model/max_pooling2d_8/MaxPool (7.74k/7.74k flops)\n",
      "  fashion_mnist_cnn_model/dense_12/MatMul (4.10k/4.10k flops)\n",
      "  fashion_mnist_cnn_model/conv2d_12/BiasAdd (2.59k/2.59k flops)\n",
      "  fashion_mnist_cnn_model/global_average_pooling2d/Mean (2.59k/2.59k flops)\n",
      "  fashion_mnist_cnn_model/dense_13/MatMul (1.28k/1.28k flops)\n",
      "  fashion_mnist_cnn_model/dense_12/BiasAdd (64/64 flops)\n",
      "  fashion_mnist_cnn_model/dense_13/Softmax (50/50 flops)\n",
      "  fashion_mnist_cnn_model/dense_13/BiasAdd (10/10 flops)\n",
      "\n",
      "======================End of Report==========================\n"
     ]
    }
   ],
   "source": [
    "flops = get_flops(model_cnn, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89e76e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.26 Million Floating Point Operations (FLOPs)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{flops / 10 ** 6:.03} Million Floating Point Operations (FLOPs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c57078",
   "metadata": {},
   "source": [
    "Here, I've greatly reduced my parameters, but not changed the FLOPs much. This change only impact the computational demands of that dense layer. I've made the input to that layer much more compact and saved a heap of parameters, but not altered the FLOPs much as most of these lie in the convolution layers.\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "Not all parameters are created equal. Convolutional kernels will incur much greater runtimes than dense layers, but with many fewer parameters. They do at least lend themelves very to paralellisation though, so at least we have that in our favour.\n",
    "\n",
    "There are a lot of other things that you can do to impact the compute demands. Some other simple things that you might want to play with include:\n",
    "* Using different strides for the convolution filters, so you don't convolve with every pixel in the input\n",
    "* Using different pooling sizes in max-pooling, such as (4x4), to more rapidly downscale the networks internal representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9295e2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
